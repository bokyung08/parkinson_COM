'''from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
#LSTM
def build_pose_model(input_shape):
    """Pose ê¸°ë°˜ LSTM ëª¨ë¸ ì •ì˜"""
    model = Sequential([
        Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape),
        Dropout(0.3),
        Bidirectional(LSTM(32)),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model
# LSTM+Attention
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional, Attention, Concatenate

def build_pose_model(input_shape):
    inputs = Input(shape=input_shape)
    x = Bidirectional(LSTM(128, return_sequences=True))(inputs)
    x = Dropout(0.3)(x)
    x2 = Bidirectional(LSTM(64, return_sequences=True))(x)
    
    # Attention
    attn_out = Attention()([x2, x2])
    concat = Concatenate()([x2, attn_out])
    x = Dense(64, activation='relu')(concat[:, -1, :])
    x = Dropout(0.4)(x)
    outputs = Dense(1, activation='sigmoid')(x)
    
    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Bidirectional, Flatten
# CNN-LSTM
def build_pose_model(input_shape):
    inputs = Input(shape=input_shape)  # (time, features)
    x = Conv1D(64, 3, activation='relu', padding='same')(inputs)
    x = MaxPooling1D(2)(x)
    x = Conv1D(128, 3, activation='relu', padding='same')(x)
    x = MaxPooling1D(2)(x)
    x = Bidirectional(LSTM(64))(x)
    x = Dropout(0.4)(x)
    x = Dense(64, activation='relu')(x)
    outputs = Dense(1, activation='sigmoid')(x)

    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Transformer Encoder
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Dropout, Add

def transformer_block(x, num_heads, key_dim, ff_dim):
    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)
    x = Add()([x, attn_output])
    x = LayerNormalization()(x)
    ff = Dense(ff_dim, activation='relu')(x)
    ff = Dense(x.shape[-1])(ff)
    x = Add()([x, ff])
    x = LayerNormalization()(x)
    return x

def build_pose_model(input_shape):
    inputs = Input(shape=input_shape)
    x = transformer_block(inputs, num_heads=4, key_dim=64, ff_dim=128)
    x = transformer_block(x, num_heads=4, key_dim=64, ff_dim=128)
    x = GlobalAveragePooling1D()(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.4)(x)
    outputs = Dense(1, activation='sigmoid')(x)

    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

    '''
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Dense, LayerNormalization, MultiHeadAttention, 
    GlobalAveragePooling1D, Dropout, Add, Layer, 
    TimeDistributed, Reshape
)


# ----------------------------------------------------------------------
# ğŸ§  Spatio-Temporal Transformer ëª¨ë¸ 
# ----------------------------------------------------------------------

# 1. Spatial-Transformer-Block
class SpatialTransformerBlock(Layer):
    """
    ê°™ì€ í”„ë ˆì„ ë‚´ì˜ ê´€ì ˆ(ë…¸ë“œ) ê°„ì˜ ê³µê°„ì  ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” ì»¤ìŠ¤í…€ ë ˆì´ì–´
    """
    def __init__(self, num_heads, key_dim, ff_dim, dropout_rate=0.1, **kwargs):
        super(SpatialTransformerBlock, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.key_dim = key_dim
        self.ff_dim = ff_dim
        self.dropout_rate = dropout_rate
        
        # ë ˆì´ì–´ ì´ˆê¸°í™”
        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)
        self.dropout_attn = Dropout(dropout_rate)
        self.add_attn = Add()
        self.norm_attn = LayerNormalization(epsilon=1e-6)
        
        self.ff_dense1 = Dense(ff_dim, activation="relu")
        self.dropout_ff = Dropout(dropout_rate)
        self.add_ff = Add()
        self.norm_ff = LayerNormalization(epsilon=1e-6)

    def build(self, input_shape):
        # build ì‹œì ì— ì¶œë ¥ ì°¨ì›ì„ ì…ë ¥ ì°¨ì›ê³¼ ë™ì¼í•˜ê²Œ ë§ì¶”ëŠ” Dense ë ˆì´ì–´ ìƒì„±
        # input_shape: (Batch, Nodes, Features)
        self.embed_dim = input_shape[-1]
        self.ff_dense2 = Dense(self.embed_dim)
        super(SpatialTransformerBlock, self).build(input_shape)

    def call(self, inputs, training=False):
        # 1. Multi-Head Attention (ê³µê°„)
        attn_output = self.mha(inputs, inputs)
        attn_output = self.dropout_attn(attn_output, training=training)
        x = self.add_attn([inputs, attn_output])
        x = self.norm_attn(x)
        
        # 2. Feed-Forward Network
        ff_output = self.ff_dense1(x)
        ff_output = self.ff_dense2(ff_output)
        ff_output = self.dropout_ff(ff_output, training=training)
        x = self.add_ff([x, ff_output])
        x = self.norm_ff(x)
        return x

    def get_config(self):
        config = super().get_config()
        config.update({
            "num_heads": self.num_heads,
            "key_dim": self.key_dim,
            "ff_dim": self.ff_dim,
            "dropout_rate": self.dropout_rate,
        })
        return config


# 2. Temporal-Transformer-Block
def temporal_transformer_block(x, num_heads, key_dim, ff_dim, dropout_rate=0.1):
    """
    ì‹œê°„ ì¶•(í”„ë ˆì„) ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡
    x shape = (Batch, num_frames, hidden_dim)
    """
    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)
    attn_output = Dropout(dropout_rate)(attn_output)
    x = Add()([x, attn_output])
    x = LayerNormalization(epsilon=1e-6)(x)
    
    ff_output = Dense(ff_dim, activation="relu")(x)
    ff_output = Dense(x.shape[-1])(ff_output) # ì…ë ¥ ì°¨ì›ê³¼ ë™ì¼í•˜ê²Œ
    ff_output = Dropout(dropout_rate)(ff_output)
    x = Add()([x, ff_output])
    x = LayerNormalization(epsilon=1e-6)(x)
    return x


# 3. train_model.pyì—ì„œ í˜¸ì¶œí•  ìµœì¢… ëª¨ë¸ ë¹Œë”
def build_pose_model(input_shape, num_heads=4, key_dim=32, ff_dim=64, num_transformer_blocks=2):
    """
    Spatio-Temporal Transformer ëª¨ë¸ ë¹Œë“œ
    input_shape = (Frames, Nodes, Features) e.g. (100, 33, 3)
    """
    # ì…ë ¥ ì°¨ì› (Frames, Nodes, Features)
    inputs = Input(shape=input_shape) 
    
    # 1. Spatial Attention
    # (Batch, Frames, Nodes, Features) -> (Batch, Frames, Nodes, EmbedDim)
    # ì„ë² ë”© ì°¨ì›ì„ MultiHeadAttention í—¤ë“œ ìˆ˜ì— ë§ê²Œ ì¡°ì •
    embed_dim = key_dim * num_heads 
    x = Dense(embed_dim)(inputs)
    
    # *** ì˜¤ë¥˜ ìˆ˜ì •ëœ ë¶€ë¶„ ***
    # 'lambda' ëŒ€ì‹  SpatialTransformerBlockì˜ *ì¸ìŠ¤í„´ìŠ¤*ë¥¼ ìƒì„±í•˜ì—¬ ì „ë‹¬
    spatial_block_instance = SpatialTransformerBlock(
        num_heads=num_heads, 
        key_dim=key_dim, 
        ff_dim=ff_dim
    )
    # TimeDistributedë¥¼ ì‚¬ìš©í•´ ê° í”„ë ˆì„(ì‹œê°„)ë³„ë¡œ Spatial-Transformerë¥¼ ë…ë¦½ ì ìš©
    spatial_x = TimeDistributed(spatial_block_instance)(x)
    
    # 2. Temporal Attention
    # (Batch, Frames, Nodes, EmbedDim) -> (Batch, Frames, Nodes*EmbedDim)
    # ì‹œê°„ ì¶•ìœ¼ë¡œ ì–´í…ì…˜ì„ ì ìš©í•˜ê¸° ìœ„í•´ ë…¸ë“œì™€ íŠ¹ì§•ì„ í¼ì¹¨
    # (spatial_x.shape[2] = Nodes, spatial_x.shape[3] = EmbedDim)
    # KerasëŠ” input_shapeì—ì„œ Batchë¥¼ ì œì™¸í•˜ë¯€ë¡œ input_shape[0]=Frames, input_shape[1]=Nodes
    x_flat = Reshape((input_shape[0], input_shape[1] * embed_dim))(spatial_x)
    
    # ì‹œê°„ ì¶• íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ ì ìš©
    temporal_x = x_flat
    for _ in range(num_transformer_blocks):
        temporal_x = temporal_transformer_block(
            temporal_x, num_heads, key_dim, ff_dim
        )
        
    # 3. Classification
    x = GlobalAveragePooling1D()(temporal_x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.4)(x)
    outputs = Dense(1, activation='sigmoid')(x)
    
    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model